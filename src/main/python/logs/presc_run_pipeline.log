"27-Dec-23 10:44:54" - root - INFO - Pipeline run_pres_pipeline is started . . 
"27-Dec-23 10:44:54" - root - INFO - main() is started . . 
"27-Dec-23 10:52:44" - root - INFO - Pipeline run_pres_pipeline is started . . 
"27-Dec-23 10:52:44" - root - INFO - main() is started . . 
"27-Dec-23 10:53:41" - root - INFO - Pipeline run_pres_pipeline is started . . 
"27-Dec-23 10:53:41" - root - INFO - main() is started . . 
"27-Dec-23 10:53:54" - run_data_ingestion - INFO - load_files() function is started . . .
"27-Dec-23 10:54:48" - root - INFO - Pipeline run_pres_pipeline is started . . 
"27-Dec-23 10:54:48" - root - INFO - main() is started . . 
"27-Dec-23 10:55:01" - run_data_ingestion - INFO - load_files() function is started . . .
"27-Dec-23 10:55:01" - run_data_ingestion - ERROR - name 'file_formater' is not defined
Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_data_ingestion.py", line 12, in load_files
    df = spark.read.format(file_formater).load(file_dir)
                           ^^^^^^^^^^^^^
NameError: name 'file_formater' is not defined. Did you mean: 'file_format'?
"27-Dec-23 10:55:01" - root - ERROR - Error in main(), check dependencies: name 'file_formater' is not defined
Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_pipeline.py", line 35, in main
    df_city = load_files(spark, file_format, file_dir, header, inferSchema)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_data_ingestion.py", line 12, in load_files
    df = spark.read.format(file_formater).load(file_dir)
                           ^^^^^^^^^^^^^
NameError: name 'file_formater' is not defined. Did you mean: 'file_format'?
"28-Dec-23 18:26:52" - root - INFO - Pipeline run_pres_pipeline is started . . 
"28-Dec-23 18:26:52" - root - INFO - main() is started . . 
"28-Dec-23 18:27:09" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 18:27:09" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 18:27:13" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 18:27:13" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 19:06:15" - root - INFO - Pipeline run_pres_pipeline is started . . 
"28-Dec-23 19:06:15" - root - INFO - main() is started . . 
"28-Dec-23 19:06:28" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 19:06:28" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 19:06:32" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 19:06:32" - run_data_ingestion - INFO - load_files() function is started . . .
"28-Dec-23 19:06:34" - run_data_preprocessing - INFO - data_preprocessing() function is started . . .
"29-Dec-23 09:24:49" - root - INFO - Pipeline run_pres_pipeline is started . . 
"29-Dec-23 09:24:49" - root - INFO - main() is started . . 
"29-Dec-23 09:25:06" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:25:06" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:25:10" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:25:10" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:25:12" - run_data_preprocessing - INFO - data_preprocessing() function is started . . .
"29-Dec-23 09:40:15" - root - INFO - Pipeline run_pres_pipeline is started . . 
"29-Dec-23 09:40:15" - root - INFO - main() is started . . 
"29-Dec-23 09:40:29" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:40:29" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:40:33" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:40:33" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:40:34" - root - INFO - Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\AppData\Local\Programs\Python\Python312\Lib\socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
"29-Dec-23 09:40:35" - root - INFO - Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
"29-Dec-23 09:40:36" - run_data_ingestion - ERROR - [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\AppData\Local\Programs\Python\Python312\Lib\socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_data_ingestion.py", line 15, in load_files
    df = spark.read.format(file_format).options(header = header, inferSchema = inferSchema).load(file_dir)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\pyspark\sql\readwriter.py", line 307, in load
    return self._df(self._jreader.load(path))
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Admin\AppData\Local\Programs\Python\Python312\Lib\logging\__init__.py", line 1529, in info
    def info(self, msg, *args, **kwargs):
    
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
"29-Dec-23 09:40:36" - root - ERROR - Error in main(), check dependencies: [WinError 10061] No connection could be made because the target machine actively refused it
Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\AppData\Local\Programs\Python\Python312\Lib\socket.py", line 707, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_pipeline.py", line 54, in main
    df_presc_fact = load_files(spark, file_format, file_dir, header, inferSchema)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_data_ingestion.py", line 15, in load_files
    df = spark.read.format(file_format).options(header = header, inferSchema = inferSchema).load(file_dir)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\pyspark\sql\readwriter.py", line 307, in load
    return self._df(self._jreader.load(path))
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 538, in send_command
    logger.info("Error while receiving.", exc_info=True)
  File "C:\Users\Admin\AppData\Local\Programs\Python\Python312\Lib\logging\__init__.py", line 1529, in info
    def info(self, msg, *args, **kwargs):
    
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\pyspark\context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\pyspark\context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\Admin\Downloads\spark-3.5.0-bin-hadoop3\python\lib\py4j-0.10.9.7-src.zip\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it
"29-Dec-23 09:41:04" - root - INFO - Pipeline run_pres_pipeline is started . . 
"29-Dec-23 09:41:04" - root - INFO - main() is started . . 
"29-Dec-23 09:41:17" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:41:17" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:41:21" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:41:21" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 09:41:22" - run_data_preprocessing - INFO - data_preprocessing() function is started . . .
"29-Dec-23 11:45:41" - root - INFO - Pipeline run_pres_pipeline is started . . 
"29-Dec-23 11:45:41" - root - INFO - main() is started . . 
"29-Dec-23 11:46:10" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 11:46:10" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 11:46:17" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 11:46:17" - run_data_ingestion - INFO - load_files() function is started . . .
"29-Dec-23 11:46:20" - run_data_preprocessing - INFO - data_preprocessing() function is started . . .
"29-Dec-23 11:46:20" - run_data_preprocessing - ERROR - 'function' object has no attribute 'partitionBy'
Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_data_preprocessing.py", line 44, in data_preprocessing
    spec = try_remote_window.partitionBy("presc_id")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'function' object has no attribute 'partitionBy'
"29-Dec-23 11:46:20" - root - ERROR - Error in main(), check dependencies: 'function' object has no attribute 'partitionBy'
Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_pipeline.py", line 58, in main
    df_city, df_presc_fact = data_preprocessing(df_city, df_presc_fact)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Admin\PycharmProjects\PrescriberAnalytics\src\main\python\bin\run_presc_data_preprocessing.py", line 44, in data_preprocessing
    spec = try_remote_window.partitionBy("presc_id")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'function' object has no attribute 'partitionBy'
